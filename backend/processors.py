"""
MemoryHunter V2.0 Pro - AI Processors
åŒ…å« MiniCPM-V 2.5 (Int4) å’Œ YOLOv8-X çš„å°è£…
"""

import torch
from PIL import Image
from typing import Dict, List, Any, Optional
from pathlib import Path
import logging

# è®¾ç½®æ—¥å¿—
logger = logging.getLogger(__name__)


class GlobalAIProcessor:
    """
    å…¨å±€AIå¤„ç†å™¨ (å•ä¾‹æ¨¡å¼)
    è´Ÿè´£åŠ è½½å’Œç®¡ç†æ‰€æœ‰å¤§æ¨¡å‹
    """
    _instance = None
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
            cls._instance._initialized = False
        return cls._instance
    
    def __init__(self):
        if self._initialized:
            return
            
        logger.info("ğŸš€ Initializing GlobalAIProcessor for V2.0 Pro...")
        
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        logger.info(f"Device: {self.device}")
        
        # æ¨¡å‹å ä½ç¬¦
        self.vlm_model = None
        self.vlm_tokenizer = None
        self.yolo_model = None
        
        self._initialized = True
        
    def load_models(self):
        """
        åŠ è½½æ‰€æœ‰æ¨¡å‹åˆ°GPU
        æŒ‰é¡ºåºåŠ è½½ä»¥ä¼˜åŒ–æ˜¾å­˜ä½¿ç”¨
        """
        try:
            # ========== åŠ è½½ YOLOv8-X ==========
            logger.info("ğŸ“¦ Loading YOLOv8-X...")
            from ultralytics import YOLO
            self.yolo_model = YOLO('yolov8x.pt')  # ä¼šè‡ªåŠ¨ä¸‹è½½
            if self.device == "cuda":
                self.yolo_model.to(self.device)
            logger.info("âœ… YOLOv8-X loaded successfully")
            
            # ========== åŠ è½½ MiniCPM-V 2.5 Int4 ==========
            logger.info("ğŸ“¦ Loading MiniCPM-V 2.5 (Int4 Quantized)...")
            from transformers import AutoModel, AutoTokenizer
            
            model_name = "openbmb/MiniCPM-V-2_5-int4"
            
            self.vlm_tokenizer = AutoTokenizer.from_pretrained(
                model_name, 
                trust_remote_code=True
            )
            
            self.vlm_model = AutoModel.from_pretrained(
                model_name,
                trust_remote_code=True,
                torch_dtype=torch.float16 if self.device == "cuda" else torch.float32
            )
            
            if self.device == "cuda":
                self.vlm_model = self.vlm_model.to(self.device)
            
            logger.info("âœ… MiniCPM-V 2.5 (Int4) loaded successfully")
            
            # æ˜¾å­˜å ç”¨æ£€æŸ¥
            if self.device == "cuda":
                allocated = torch.cuda.memory_allocated() / 1024**3
                reserved = torch.cuda.memory_reserved() / 1024**3
                logger.info(f"ğŸ’¾ GPU Memory: Allocated={allocated:.2f}GB, Reserved={reserved:.2f}GB")
                
        except Exception as e:
            logger.error(f"Failed to load models: {e}")
            raise
    
    def detect_objects(self, image_path: str, conf_threshold: float = 0.3) -> List[Dict[str, Any]]:
        """
        ä½¿ç”¨ YOLOv8 æ£€æµ‹å›¾ç‰‡ä¸­çš„ç‰©ä½“
        
        Args:
            image_path: å›¾ç‰‡è·¯å¾„
            conf_threshold: ç½®ä¿¡åº¦é˜ˆå€¼
            
        Returns:
            [{label: str, box: [x1, y1, x2, y2], score: float}, ...]
        """
        if self.yolo_model is None:
            logger.warning("YOLO model not loaded, skipping object detection")
            return []
        
        try:
            results = self.yolo_model(image_path, conf=conf_threshold, verbose=False)
            
            objects = []
            for result in results:
                boxes = result.boxes
                for box in boxes:
                    # YOLO è¿”å›çš„åæ ‡æ ¼å¼: xyxy
                    x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
                    conf = float(box.conf[0])
                    cls_id = int(box.cls[0])
                    label = result.names[cls_id]
                    
                    objects.append({
                        "label": label,
                        "box": [float(x1), float(y1), float(x2), float(y2)],
                        "score": conf
                    })
            
            logger.debug(f"Detected {len(objects)} objects in {Path(image_path).name}")
            return objects
            
        except Exception as e:
            logger.error(f"Object detection failed: {e}")
            return []
    
    def analyze_image_with_vlm(self, image_path: str) -> Dict[str, str]:
        """
        ä½¿ç”¨ MiniCPM-V åˆ†æå›¾ç‰‡,ç”Ÿæˆæè¿°å’ŒOCR
        
        Args:
            image_path: å›¾ç‰‡è·¯å¾„
            
        Returns:
            {caption: str, ocr_text: str}
        """
        if self.vlm_model is None or self.vlm_tokenizer is None:
            logger.warning("VLM model not loaded, returning empty analysis")
            return {"caption": "", "ocr_text": ""}
        
        try:
            image = Image.open(image_path).convert('RGB')
            
            # Prompt è®¾è®¡: è¦æ±‚è¯¦ç»†æè¿° + OCR
            question = """è¯·è¯¦ç»†åˆ†æè¿™å¼ å›¾ç‰‡:
1. æè¿°ç”»é¢ä¸­çš„ä¸»è¦å†…å®¹ã€ç‰©ä½“ã€äººç‰©ã€åŠ¨ä½œã€åœºæ™¯å’Œæ°›å›´
2. å¦‚æœå›¾ç‰‡ä¸­åŒ…å«æ–‡å­—,è¯·å®Œæ•´æå–å‡ºæ¥

è¯·æŒ‰ç…§ä»¥ä¸‹æ ¼å¼å›ç­”:
ã€æè¿°ã€‘...
ã€æ–‡å­—ã€‘...(å¦‚æœæ²¡æœ‰æ–‡å­—åˆ™å†™"æ— ")"""
            
            # è°ƒç”¨ VLM (å‚è€ƒ MiniCPM-V å®˜æ–¹API)
            msgs = [{'role': 'user', 'content': question}]
            
            # ç”Ÿæˆå›ç­”
            response = self.vlm_model.chat(
                image=image,
                msgs=msgs,
                tokenizer=self.vlm_tokenizer,
                sampling=True,
                temperature=0.7
            )
            
            # è§£æè¾“å‡º
            caption = ""
            ocr_text = ""
            
            if "ã€æè¿°ã€‘" in response:
                parts = response.split("ã€æè¿°ã€‘")
                if len(parts) > 1:
                    desc_part = parts[1].split("ã€æ–‡å­—ã€‘")[0].strip()
                    caption = desc_part
                    
                if "ã€æ–‡å­—ã€‘" in response:
                    ocr_part = response.split("ã€æ–‡å­—ã€‘")[1].strip()
                    if ocr_part and ocr_part != "æ— ":
                        ocr_text = ocr_part
            else:
                # å…œåº•: å¦‚æœè§£æå¤±è´¥,æ•´ä¸ªå›ç­”ä½œä¸ºæè¿°
                caption = response.strip()
            
            logger.debug(f"VLM analysis complete for {Path(image_path).name}")
            return {"caption": caption, "ocr_text": ocr_text}
            
        except Exception as e:
            logger.error(f"VLM analysis failed: {e}")
            return {"caption": "", "ocr_text": ""}
    
    def process_image(self, image_path: str) -> Dict[str, Any]:
        """
        å®Œæ•´å¤„ç†ä¸€å¼ å›¾ç‰‡: YOLO + VLM
        
        Args:
            image_path: å›¾ç‰‡è·¯å¾„
            
        Returns:
            {
                objects: List[{label, box, score}],
                caption: str,
                ocr_text: str
            }
        """
        logger.info(f"Processing image: {Path(image_path).name}")
        
        # Step 1: ç‰©ä½“æ£€æµ‹
        objects = self.detect_objects(image_path)
        
        # Step 2: VLM æ·±åº¦åˆ†æ
        vlm_result = self.analyze_image_with_vlm(image_path)
        
        return {
            "objects": objects,
            "caption": vlm_result["caption"],
            "ocr_text": vlm_result["ocr_text"]
        }


# å…¨å±€å•ä¾‹å®ä¾‹
_global_processor: Optional[GlobalAIProcessor] = None


def get_processor() -> GlobalAIProcessor:
    """è·å–å…¨å±€å¤„ç†å™¨å®ä¾‹"""
    global _global_processor
    if _global_processor is None:
        _global_processor = GlobalAIProcessor()
        _global_processor.load_models()
    return _global_processor
