"""
Mini-CPM-V VLM (è§†è§‰è¯­è¨€æ¨¡å‹) ç®¡ç†å™¨

æä¾›å›¾ç‰‡æ·±åº¦ç†è§£èƒ½åŠ›:
- OCR æ–‡å­—æå– (å‡†ç¡®ç‡ 90%+)
- åœºæ™¯æè¿°ç”Ÿæˆ
- ç‰©ä½“å…³ç³»ç†è§£
- æ™ºèƒ½æ ‡ç­¾ç”Ÿæˆ

ä¼˜åŒ–ç­–ç•¥:
- INT8 é‡åŒ– (å†…å­˜å‡åŠ)
- æ‰¹å¤„ç† (æå‡æ•ˆç‡)
- æ‡’åŠ è½½ (æŒ‰éœ€ä½¿ç”¨)
"""

import torch
from transformers import AutoModel, AutoTokenizer
from PIL import Image
import logging
from typing import List, Dict, Optional
import gc

logger = logging.getLogger(__name__)


# ============ å®˜æ–¹ä¿®å¤ï¼šPatch flash_attn ä¾èµ– ============
# æ¥æº: https://huggingface.co/openbmb/MiniCPM-V-2_6/discussions
# ç›®çš„: åœ¨ CPU ç¯å¢ƒç»•è¿‡ flash_attn æ£€æŸ¥
def _patch_flash_attn():
    """
    Patch transformers to skip flash_attn import check
    
    This is the official workaround from HuggingFace for running
    models with flash_attn dependencies on CPU-only environments.
    """
    import transformers.dynamic_module_utils
    import importlib.util
    
    # ä¿å­˜åŸå§‹å‡½æ•°
    original_get_imports = transformers.dynamic_module_utils.get_imports
    
    def custom_get_imports(filename: str | os.PathLike) -> list[str]:
        """ä¿®æ”¹åçš„ get_importsï¼šç§»é™¤ flash_attn"""
        imports = original_get_imports(filename)
        
        # è¿‡æ»¤æ‰ flash_attnï¼ˆCPU ä¸éœ€è¦ï¼‰
        filtered_imports = [imp for imp in imports if imp != "flash_attn"]
        
        if len(filtered_imports) < len(imports):
            logger.info("  - âœ… å·²ç»•è¿‡ flash_attn ä¾èµ–æ£€æŸ¥ï¼ˆCPU ä¼˜åŒ–ï¼‰")
        
        return filtered_imports
    
    # åº”ç”¨ patch
    transformers.dynamic_module_utils.get_imports = custom_get_imports

# åº”ç”¨ä¿®å¤
import os
_patch_flash_attn()


class MiniCPMVManager:
    """Mini-CPM-V æ¨¡å‹ç®¡ç†å™¨
    
    å•ä¾‹æ¨¡å¼ï¼Œç¡®ä¿æ¨¡å‹åªåŠ è½½ä¸€æ¬¡
    æ”¯æŒ INT8 é‡åŒ–å’Œæ‡’åŠ è½½
    """
    
    _instance = None
    
    def __new__(cls):
        if cls._instance is None:
            cls._instance = super(MiniCPMVManager, cls).__new__(cls)
            cls._instance._initialized = False
        return cls._instance
    
    def __init__(self):
        if self._initialized:
            return
        
        self.model = None
        self.tokenizer = None
        self.device = "cpu"  # Docker ç¯å¢ƒåªèƒ½ç”¨ CPU
        self._initialized = True
        
        logger.info("âœ… MiniCPMV Manager åˆå§‹åŒ–å®Œæˆ")
    
    def load_model(self, model_name: str = "openbmb/MiniCPM-V-2_6", use_quantization: bool = True):
        """åŠ è½½æ¨¡å‹
        
        Args:
            model_name: æ¨¡å‹åç§°
            use_quantization: æ˜¯å¦ä½¿ç”¨ INT8 é‡åŒ–ï¼ˆæ¨èå¼€å¯ï¼‰
        """
        if self.model is not None:
            logger.info("æ¨¡å‹å·²åŠ è½½ï¼Œè·³è¿‡é‡å¤åŠ è½½")
            return
        
        try:
            logger.info(f"ğŸš€ å¼€å§‹åŠ è½½ {model_name} æ¨¡å‹...")
            logger.info(f"  - é‡åŒ–: {'INT8' if use_quantization else 'å…³é—­'}")
            logger.info(f"  - è®¾å¤‡: {self.device}")
            
            # è·å– HuggingFace Tokenï¼ˆå¦‚æœæœ‰ï¼‰
            import os
            hf_token = os.environ.get('HF_TOKEN')
            if hf_token:
                logger.info("  - âœ… ä½¿ç”¨ HuggingFace Token è®¿é—®æ¨¡å‹")
            
            # ç¦ç”¨ Flash Attention æ£€æŸ¥ï¼ˆCPU ç¯å¢ƒä¸éœ€è¦ï¼‰
            os.environ['DISABLE_FLASH_ATTN'] = '1'
            
            # INT8 é‡åŒ–é…ç½®ï¼ˆå‡å°‘å†…å­˜å ç”¨ï¼‰
            load_kwargs = {
                "trust_remote_code": True,
                "torch_dtype": torch.float16 if not use_quantization else torch.float32,
                "low_cpu_mem_usage": True,  # CPU ä¼˜åŒ–
                "device_map": "cpu",  # å¼ºåˆ¶ CPU
            }
            
            # æ·»åŠ  tokenï¼ˆå¦‚æœæœ‰ï¼‰
            if hf_token:
                load_kwargs["token"] = hf_token
            
            if use_quantization:
                try:
                    from transformers import BitsAndBytesConfig
                    quantization_config = BitsAndBytesConfig(
                        load_in_8bit=True,
                        llm_int8_threshold=6.0,
                    )
                    load_kwargs["quantization_config"] = quantization_config
                    logger.info("  - âœ… INT8 é‡åŒ–å·²å¯ç”¨ (å†…å­˜å ç”¨å‡åŠ)")
                except ImportError:
                    logger.warning("  - âš ï¸ bitsandbytes æœªå®‰è£…ï¼Œè·³è¿‡é‡åŒ–")
            
            # åŠ è½½æ¨¡å‹
            self.model = AutoModel.from_pretrained(
                model_name,
                **load_kwargs
            )
            
            # ç§»åˆ° CPUï¼ˆDocker ç¯å¢ƒï¼‰
            if not use_quantization:
                self.model = self.model.to(self.device)
            
            self.model.eval()  # è¯„ä¼°æ¨¡å¼
            
            # åŠ è½½ tokenizer
            tokenizer_kwargs = {"trust_remote_code": True}
            if hf_token:
                tokenizer_kwargs["token"] = hf_token
                
            self.tokenizer = AutoTokenizer.from_pretrained(
                model_name,
                **tokenizer_kwargs
            )
            
            logger.info("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ!")
            
        except Exception as e:
            logger.error(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise
    
    def analyze_image(
        self,
        image: Image.Image,
        prompt: Optional[str] = None,
        max_new_tokens: int = 512
    ) -> Dict[str, any]:
        """æ·±åº¦åˆ†æå›¾ç‰‡
        
        Args:
            image: PIL Image å¯¹è±¡
            prompt: è‡ªå®šä¹‰æç¤ºè¯ï¼ˆå¯é€‰ï¼‰
            max_new_tokens: æœ€å¤§ç”Ÿæˆ token æ•°
            
        Returns:
            åŒ…å«åˆ†æç»“æœçš„å­—å…¸
        """
        if self.model is None:
            raise RuntimeError("æ¨¡å‹æœªåŠ è½½ï¼Œè¯·å…ˆè°ƒç”¨ load_model()")
        
        # é»˜è®¤ Promptï¼ˆé’ˆå¯¹ç›¸å†Œåœºæ™¯ä¼˜åŒ–ï¼‰
        if prompt is None:
            prompt = self._get_default_prompt()
        
        try:
            # å‡†å¤‡è¾“å…¥
            msgs = [{'role': 'user', 'content': prompt}]
            
            # ç”Ÿæˆå›å¤
            with torch.no_grad():
                res = self.model.chat(
                    image=image,
                    msgs=msgs,
                    tokenizer=self.tokenizer,
                    max_new_tokens=max_new_tokens,
                    sampling=False,  # ç¡®å®šæ€§è¾“å‡º
                )
            
            # è§£æç»“æœ
            return self._parse_result(res)
            
        except Exception as e:
            logger.error(f"åˆ†æå›¾ç‰‡æ—¶å‡ºé”™: {e}")
            return {
                "description": "",
                "ocr_text": "",
                "tags": [],
                "error": str(e)
            }
    
    def batch_analyze(
        self,
        images: List[Image.Image],
        batch_size: int = 2
    ) -> List[Dict[str, any]]:
        """æ‰¹é‡åˆ†æå›¾ç‰‡
        
        Args:
            images: å›¾ç‰‡åˆ—è¡¨
            batch_size: æ‰¹å¤„ç†å¤§å°ï¼ˆ8GBå†…å­˜å»ºè®®2ï¼‰
            
        Returns:
            åˆ†æç»“æœåˆ—è¡¨
        """
        results = []
        
        for i in range(0, len(images), batch_size):
            batch = images[i:i+batch_size]
            logger.info(f"å¤„ç†æ‰¹æ¬¡ {i//batch_size + 1}/{(len(images)-1)//batch_size + 1}")
            
            for image in batch:
                result = self.analyze_image(image)
                results.append(result)
            
            # æ¸…ç†å†…å­˜
            if i % (batch_size * 5) == 0:
                gc.collect()
        
        return results
    
    def _get_default_prompt(self) -> str:
        """è·å–é»˜è®¤ Promptï¼ˆé’ˆå¯¹ç›¸å†Œåœºæ™¯ä¼˜åŒ–ï¼‰"""
        return """è¯·è¯¦ç»†æè¿°è¿™å¼ å›¾ç‰‡ï¼ŒåŒ…æ‹¬ï¼š

1. **ä¸»è¦å†…å®¹**: æè¿°å›¾ç‰‡ä¸­çš„ä¸»è¦ç‰©ä½“ã€äººç‰©å’Œåœºæ™¯
2. **æ–‡å­—ä¿¡æ¯**: æå–æ‰€æœ‰å¯è§çš„æ–‡å­—ï¼ŒåŒ…æ‹¬æ ‡å¿—ã€æ‹›ç‰Œã€æ–‡æ¡£ã€å±å¹•æ˜¾ç¤ºç­‰
3. **è§†è§‰ç‰¹å¾**: ä¸»è¦é¢œè‰²ã€å…‰çº¿ã€æ„å›¾
4. **æ ‡ç­¾**: ç”¨ç®€çŸ­çš„è¯è¯­æ ‡æ³¨å›¾ç‰‡ç±»å‹ï¼ˆå¦‚: é£æ™¯ã€ç¾é£Ÿã€æ–‡æ¡£ã€äººç‰©ç­‰ï¼‰

è¯·ç”¨ä¸­æ–‡å›ç­”ï¼Œæ ¼å¼æ¸…æ™°ã€‚"""
    
    def _parse_result(self, raw_result: str) -> Dict[str, any]:
        """è§£ææ¨¡å‹è¾“å‡º
        
        æå–: æè¿°ã€OCRæ–‡å­—ã€æ ‡ç­¾
        """
        result = {
            "description": raw_result,
            "ocr_text": "",
            "tags": [],
            "raw": raw_result
        }
        
        # å°è¯•æå– OCR æ–‡å­—
        if "æ–‡å­—ä¿¡æ¯" in raw_result or "æ–‡å­—" in raw_result:
            # ç®€å•çš„å¯å‘å¼æå–
            lines = raw_result.split('\n')
            ocr_lines = []
            capturing = False
            
            for line in lines:
                if "æ–‡å­—" in line or "OCR" in line:
                    capturing = True
                    continue
                if capturing and line.strip():
                    if line.startswith('#') or "æ ‡ç­¾" in line:
                        break
                    ocr_lines.append(line.strip())
            
            result["ocr_text"] = " ".join(ocr_lines)
        
        # å°è¯•æå–æ ‡ç­¾
        if "æ ‡ç­¾" in raw_result:
            lines = raw_result.split('\n')
            for line in lines:
                if "æ ‡ç­¾" in line:
                    # æå–æ ‡ç­¾ï¼ˆé€—å·æˆ–é¡¿å·åˆ†éš”ï¼‰
                    tag_part = line.split(':')[-1].split('ï¼š')[-1]
                    tags = [t.strip() for t in tag_part.replace('ã€', ',').split(',')]
                    result["tags"] = [t for t in tags if t]
        
        return result
    
    def unload_model(self):
        """å¸è½½æ¨¡å‹é‡Šæ”¾å†…å­˜"""
        if self.model is not None:
            del self.model
            del self.tokenizer
            self.model = None
            self.tokenizer = None
            gc.collect()
            logger.info("âœ… æ¨¡å‹å·²å¸è½½ï¼Œå†…å­˜å·²é‡Šæ”¾")
    
    def __del__(self):
        """ææ„å‡½æ•°ï¼šç¡®ä¿èµ„æºè¢«é‡Šæ”¾"""
        self.unload_model()
