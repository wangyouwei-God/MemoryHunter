"""
å›¾ç‰‡ç´¢å¼•å™¨ - V2.0 Pro (CLIP + VLM + YOLO)
æ‰«æç›¸å†Œç›®å½•å¹¶æå–å¤šç»´åº¦ç‰¹å¾
"""

from PIL import Image
from pillow_heif import register_heif_opener
import logging
import json
from pathlib import Path
from typing import List, Callable, Optional
import gc

from .config import (
    PHOTOS_DIR, SUPPORTED_FORMATS,
    ENABLE_VLM, ENABLE_OBJECT_DETECTION
)

# æ³¨å†Œ HEIC æ ¼å¼æ”¯æŒ
register_heif_opener()

logger = logging.getLogger(__name__)


class ImageIndexer:
    """å›¾ç‰‡ç´¢å¼•å™¨ - V2.0 Pro"""
    
    def __init__(self, visual_model, vector_db, semantic_model=None, ai_processor=None):
        """
        åˆå§‹åŒ–ç´¢å¼•å™¨ - V2.0 Pro
        
        Args:
            visual_model: CLIPModelManager å®ä¾‹ (è§†è§‰ç¼–ç )
            vector_db: VectorDatabase å®ä¾‹ (åŒé›†åˆ)
            semantic_model: (Optional) BGEModelManager å®ä¾‹ (è¯­ä¹‰ç¼–ç )
            ai_processor: (Optional) GlobalAIProcessor å®ä¾‹ (VLM + YOLO)
        """
        self.visual_model = visual_model
        self.db = vector_db
        self.semantic_model = semantic_model
        self.ai_processor = ai_processor
        self.logger = logging.getLogger(__name__)
        
        # æ ¹æ®é…ç½®æ˜¾ç¤ºæ¨¡å¼
        if ENABLE_VLM and self.ai_processor:
            self.logger.info("ğŸš€ V2.0 Pro æ¨¡å¼: CLIP + VLM + YOLO")
        else:
            self.logger.info("ğŸ“Œ V1.0 å…¼å®¹æ¨¡å¼: ä»… CLIP")
    
    def scan_photos(self) -> List[Path]:
        """æ‰«æç›¸å†Œç›®å½•"""
        photos = []
        if not PHOTOS_DIR.exists():
            self.logger.warning(f"ç›¸å†Œç›®å½•ä¸å­˜åœ¨: {PHOTOS_DIR}")
            return photos
        
        self.logger.info(f"æ­£åœ¨æ‰«æç›®å½•: {PHOTOS_DIR}")
        for photo_path in PHOTOS_DIR.rglob("*"):
            if photo_path.is_file() and photo_path.suffix in SUPPORTED_FORMATS:
                photos.append(photo_path)
        
        self.logger.info(f"âœ… æ‰¾åˆ° {len(photos)} å¼ å›¾ç‰‡")
        return photos
    
    def index_all(self, progress_callback: Optional[Callable[[int, int], None]] = None) -> dict:
        """ç´¢å¼•æ‰€æœ‰å›¾ç‰‡ (æ”¯æŒå–æ¶ˆ)"""
        photos = self.scan_photos()
        total = len(photos)
        
        if total == 0:
            return {'total': 0, 'success': 0, 'failed': 0, 'skipped': 0}
        
        success_count = 0
        failed_count = 0
        skipped_count = 0
        
        self.logger.info(f"å¼€å§‹ç´¢å¼• {total} å¼ å›¾ç‰‡...")
        
        for i, photo_path in enumerate(photos):
            # æ£€æŸ¥å–æ¶ˆæ ‡å¿— (ä»main.pyå¯¼å…¥)
            try:
                from .main import cancel_indexing_flag
                if cancel_indexing_flag:
                    self.logger.info(f"ç´¢å¼•å·²è¢«ç”¨æˆ·å–æ¶ˆ (å¤„ç†äº† {i}/{total} å¼ )")
                    break
            except ImportError:
                pass  # å•å…ƒæµ‹è¯•æ—¶å¯èƒ½æ— æ³•å¯¼å…¥
            
            try:
                # æ£€æŸ¥æ˜¯å¦å·²ç´¢å¼•
                if self.db.check_image_exists(str(photo_path)):
                    skipped_count += 1
                    if progress_callback: progress_callback(i + 1, total)
                    continue
                
                # ç´¢å¼•å•å¼ å›¾ç‰‡
                if self._index_single_internal(photo_path):
                    success_count += 1
                    if (i+1) % 10 == 0:
                        self.logger.info(f"è¿›åº¦: {i+1}/{total}")
                else:
                    failed_count += 1
                
            except Exception as e:
                failed_count += 1
                self.logger.warning(f"å¤„ç†å¤±è´¥ {photo_path.name}: {e}")
                
            finally:
                if progress_callback: progress_callback(i + 1, total)
                
                # V2.0 Pro: æ¯å¼ å›¾å¤„ç†å®Œåæ¸…ç†GPUç¼“å­˜
                if self.ai_processor and hasattr(self.ai_processor, 'device') and self.ai_processor.device == "cuda":
                    import torch
                    torch.cuda.empty_cache()
                
                # å®šæœŸæ¸…ç†å†…å­˜
                if (i + 1) % 20 == 0:
                    gc.collect()
        
        return {
            'total': total,
            'success': success_count,
            'failed': failed_count,
            'skipped': skipped_count
        }
    
    def _index_single_internal(self, photo_path: Path) -> bool:
        """
        ç´¢å¼•å•å¼ å›¾ç‰‡ (å¤šæ¨¡å‹æµæ°´çº¿)
        
        æµç¨‹:
        1. CLIPè§†è§‰ç¼–ç  (å¿…é¡»)
        2. å¦‚æœå¯ç”¨VLM: VLMæ·±åº¦åˆ†æ + YOLOç‰©ä½“æ£€æµ‹
        3. å¦‚æœæœ‰VLMç»“æœ: BGEè¯­ä¹‰ç¼–ç 
        4. å­˜å…¥åŒé›†åˆæ•°æ®åº“
        """
        try:
            image = Image.open(photo_path).convert("RGB")
            
            # ========== Step 1: CLIP è§†è§‰ç¼–ç  (å¿…é¡») ==========
            visual_embedding = self.visual_model.encode_image(image)
            
            # åŸºç¡€å…ƒæ•°æ®
            base_metadata = {
                'path': str(photo_path),
                'filename': photo_path.name,
                'vlm_analyzed': False
            }
            
            # ========== Step 2: Pro æ¨¡å¼å¤„ç† ==========
            semantic_embedding = None
            pro_metadata = None
            
            if ENABLE_VLM and self.ai_processor and self.semantic_model:
                try:
                    # 2a. VLM + YOLO åˆ†æ
                    self.logger.debug(f"ğŸ§  VLM analyzing: {photo_path.name}")
                    ai_result = self.ai_processor.process_image(str(photo_path))
                    
                    # 2b. æ„å»ºProå…ƒæ•°æ®
                    caption = ai_result.get('caption', '')
                    ocr_text = ai_result.get('ocr_text', '')
                    objects = ai_result.get('objects', [])
                    
                    # å°†objectsåºåˆ—åŒ–ä¸ºJSONå­—ç¬¦ä¸² (ChromaDBä¸æ”¯æŒåµŒå¥—å¯¹è±¡)
                    objects_json = json.dumps(objects, ensure_ascii=False)
                    
                    pro_metadata = {
                        'caption': caption,
                        'ocr_text': ocr_text,
                        'objects': objects_json,
                        'vlm_analyzed': True
                    }
                    
                    # 2c. è¯­ä¹‰å‘é‡åŒ– (å¯¹caption + ocr_text)
                    semantic_text = f"{caption} {ocr_text}".strip()
                    if semantic_text:
                        semantic_embedding = self.semantic_model.encode_text(semantic_text)
                    
                    self.logger.debug(f"âœ… Pro analysis complete: {photo_path.name}")
                    
                except Exception as e:
                    self.logger.warning(f"Proå¤„ç†å¤±è´¥ (å°†å›é€€åˆ°V1.0): {e}")
                    # å‡ºé”™æ—¶å›é€€åˆ°V1.0æ¨¡å¼
                    pro_metadata = None
                    semantic_embedding = None
            
            # ========== Step 3: å­˜å…¥æ•°æ®åº“ ==========
            if semantic_embedding is not None and pro_metadata is not None:
                # Proæ¨¡å¼: åŒå‘é‡å­˜å‚¨
                self.db.add_image(
                    path=str(photo_path),
                    visual_embedding=visual_embedding.tolist(),
                    metadata=base_metadata,
                    semantic_embedding=semantic_embedding.tolist(),
                    pro_metadata=pro_metadata
                )
            else:
                # V1.0å…¼å®¹æ¨¡å¼: ä»…è§†è§‰å‘é‡
                self.db.add_image(
                    path=str(photo_path),
                    visual_embedding=visual_embedding.tolist(),
                    metadata=base_metadata
                )
            
            return True
            
        except Exception as e:
            self.logger.error(f"ç´¢å¼•å¤±è´¥ {photo_path}: {e}")
            return False
    
    def index_single(self, photo_path: Path) -> bool:
        """å¯¹å¤–æš´éœ²çš„å•å›¾ç´¢å¼•æ¥å£"""
        return self._index_single_internal(photo_path)